{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports to carry out numerical analysis and visualisation:\n",
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt # for crude plots without annotations just for my own checking not publication grade\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "import plotly.io as pio\n",
    "import glob\n",
    "plotly.offline.init_notebook_mode()\n",
    "from analysis_functions import boxplot_grouped, grouped_plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "The Department for Big Lorries (DfBL) is responsible for ensuring that the goods vehicles used on the nationâ€™s roads do not pose a danger to their passengers, and to other road users. Every year since 2000 the DfBL has conducted inspections on a sample of road-going vehicles. \n",
    "\n",
    "The DfBL does not inspect the same vehicles every year, but instead chooses a sample of the vehicles on the road. As a rule, the DfBL aim to inspect each vehicle on the road at least once every 12 years, and inspect most once every 6 years. The DfBL inspects vehicles previously found to be in poor condition more often, and inspects those previously found to be in good condition less often. \n",
    "\n",
    "The inspection categorises the vehicles into three types (heavy goods vehicles, light goods vehicles, and personnel), into the ten most common manufacturers, and then grades the vehicles on a 100-point scale, where 100 is perfect and 0 represents a vehicle in hazardous condition.\n",
    "\n",
    "The attached spreadsheet (VehicleData.xls) contains the results of the inspections from 2000 onwards. The fields in the data are as follows:\n",
    "\n",
    "    1. The unique ID of the vehicle;\n",
    "    2. The type of vehicle;\n",
    "    3. The manufacturer;\n",
    "    4. The inspection score;\n",
    "The client hands you this data and asks you to investigate the following questions:\n",
    "\n",
    "    1. How does the sample size vary from year-to-year? Are there any years that the DfBL should exclude from this analysis?\n",
    "    2. Assuming that the data is a representative sample of the population; does the condition of road-going vehicles appear to be changing over time? If so, is it deteriorating or improving?\n",
    "    3. Are there any particular vehicle types or manufacturers that the DfBL should pay special attention to? Would you suggest any changes to the sampling methodology to obtain better information for these types?\n",
    "    4. Since 2000 the vehicle inspectors have slowly improved the algorithm that they use to determine which vehicles to inspect more frequently, so that they are getting better at inspecting vehicles in poor condition more often. What effect could this have on the results?\n",
    "    5. Is DfBL conducting a sufficient number of inspections a year? Explain and justify your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imports\n",
    "\n",
    "Data is read from flat files and excel sheets into pandas DataFrames that are concatenated into one for further analysis in this exercise. Before specific questions are tackled some exploratory analysis is carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a quick import paths are manually added:\n",
    "\n",
    "file1 = pandas.read_csv(r'./Data/VehicleData_csv.csv')\n",
    "file2 = pandas.read_excel(r'./Data/VehicleData_2003.xls')\n",
    "file3 = pandas.read_excel(r'./Data/VehicleData_2010.xlsx')\n",
    "file = pandas.concat([file1, file2,file3])\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any null values?:\n",
    "\n",
    "null_exist = file.isnull().values.any()\n",
    "print('Do nan values exist: ', null_exist)\n",
    "\n",
    "string_vals = type(file[['Manufacturer','FinancialYear', 'VehicleType']].values.any())!=str\n",
    "print('Do wrong strings exist: ', string_vals)\n",
    "\n",
    "print('Total number of records: ', file.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of unique vehicles sampled during the whole 12 year period (population size):\n",
    "vehicle_count = file.groupby('VehicleID').count().iloc[0:-1, 1].shape[0]\n",
    "print('Total number of unique vehicles sampled: ', vehicle_count)\n",
    "\n",
    "# the frequency of most and least checked vehicles:\n",
    "min_checked_count = file.groupby('VehicleID').size().min()\n",
    "max_checked_count = file.groupby('VehicleID').size().max()\n",
    "print(f'min and max check counts are: {min_checked_count} and {max_checked_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "The size of a population 20 508 (the count of unique vehicle ids in the whole dataframe) because it was said that every vehicle is inspected at least once every 12 years and investion report covers 12 years. We do not know that is a score distribution of population. Our sampling is biased and sample distribution is not the same as population distribution. We are heavily biased to take sample from the low score tail of population distribution.\n",
    "\n",
    "To check that most vehicles are checked at least every 6 years we should get at least 2 records per vehicle id for most vehicle id's. Indeed the minimum amount each vehicle was checked is 3 amd maximum is 24. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a Dataframe for annual sample size, exponentially weighted mean sample\n",
    "# size (where damping is eqiuivalent of 3 years window) and exponentially weighted\n",
    "# standard deviation of the same window size~\n",
    "\n",
    "count_stats = file.groupby('FinancialYear').size().reset_index()\n",
    "count_stats.columns = ['FinancialYear', 'SampleSize']\n",
    "count_stats['exp_weighed_mean_sample_size'] = count_stats.SampleSize.ewm(span=3, adjust=False).mean()\n",
    "count_stats['exp_weighed_std_sample_size'] = count_stats.SampleSize.ewm(span=3, adjust=False).std()\n",
    "\n",
    "count_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot annual sample size as a scatter plot, also +1/-1 STD around it as a shaded region\n",
    "# and an exponentially weighted rolling mean as a thick line:\n",
    "# Plotting in this cell is not wrapped under a function as I\n",
    "# hardly see it can be reused in the same fashion\n",
    "\n",
    "upper_bound = go.Scatter(\n",
    "    name='Exponentially weighted standard deviation',\n",
    "    x=count_stats.FinancialYear,\n",
    "    y=count_stats.SampleSize+count_stats.exp_weighed_std_sample_size,\n",
    "    mode = 'lines',\n",
    "    line=dict(width=0.5,\n",
    "              color='rgb(131, 90, 241)'),\n",
    "    fill='tonexty')\n",
    "    \n",
    "\n",
    "lower_bound = go.Scatter(\n",
    "    name='Lower Bound',\n",
    "    x=count_stats.FinancialYear,\n",
    "    y=count_stats.SampleSize-count_stats.exp_weighed_std_sample_size,\n",
    "    showlegend=False,\n",
    "    mode = 'lines',\n",
    "    line=dict(width=0.5,\n",
    "              color='rgb(131, 90, 241)'))\n",
    "\n",
    "trace = go.Scatter(\n",
    "    name='Actual sample size',\n",
    "    x=count_stats.FinancialYear,\n",
    "    y=count_stats.SampleSize,\n",
    "    showlegend=True,\n",
    "    mode = 'markers')\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    name='Exponentially weighted rolling mean',\n",
    "    x=count_stats.FinancialYear,\n",
    "    y=count_stats.exp_weighed_mean_sample_size,\n",
    "    mode = 'lines',\n",
    "    line=dict(width=1.5,\n",
    "              color='navy'))\n",
    "\n",
    "\n",
    "data = [trace, lower_bound, upper_bound, trace2]\n",
    "\n",
    "layout= go.Layout(\n",
    "    title= None,\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title= 'Financial year',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Count (number of vehicles)',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    legend=dict(orientation=\"h\",\n",
    "               x=0, y=-0.3)\n",
    "    )\n",
    "fig= go.Figure(data=data, layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "# Export this plot as a vector graphic image:\n",
    "pio.write_image(fig, './images/sample_size_variation.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that sample size in year 2000 is 4.3 times smaller than the upcoming year. Taking sample sizes for the 12 year period into consideration this clearly is a candidate for an outlier. Yet can it be used for further analysis? We should not use the year 2000 data, because it does not generalise to the population statistics the same way the other year values do and we can therefore not compare insights drawn in 2000 to subsequent years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing records from the year 2000-2001:\n",
    "\n",
    "file = file[file.FinancialYear!='2000-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "\n",
    "First let's check a very crude trend of overall ConditionScore each year, where a bigger value indicates greater condition. We see that this crude way shows that each year vehicles score from 44 to 52. The spread is very similar both in terms of STD or MAD and we can not yet say anything about the shape of the distribution. Maybe I'll have to histogram it for a few years chosen see how they compare in shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what is the overall condition score and mean absolute deviation\n",
    "# each year and assign it to the variable \"annual_condition\":\n",
    "\n",
    "annual_condition = file.groupby('FinancialYear').ConditionScore.agg(['mean', 'mad','std']).reset_index()\n",
    "annual_condition.columns = ['FinancialYear', 'mean_score', 'mad_score', 'std_score']\n",
    "annual_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly check how a boxplot of means where all vehicle types are binned\n",
    "# togeter looks like:\n",
    "\n",
    "file.boxplot(column='ConditionScore', by='FinancialYear', rot=45, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks that boxplotting records with really large 1-3 interquartile spread range spoils the visual effect of median (default pandas setting for green line) score value trend throughtout years. There is no particular need to use STD, because we can not infer confidence intervals: condition score values are not supposed to be normally distributed per each year. Therefore MAD can equally be used as it is less prone to be biased by outliers (L1 vs L2 type of loss). Let's make a nicer plotly figure with a boxploted scores for each year but grouped by vehicle type. This will illustrate trend in both mean value and the spread of values around it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a dataframe \"mean_ConditionScore\" that summarizes mean score for each\n",
    "# type of vehicle every year:\n",
    "\n",
    "pivot_ConditionScore = file.pivot_table(values='ConditionScore', index='FinancialYear', columns='VehicleType', aggfunc=['mean','mad']).reset_index()\n",
    "pivot_ConditionScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot each year's score grouped by vehicle type using imported function where I factored out some\n",
    "# functions I defined myself:\n",
    "\n",
    "colors = ['rgb(17,30,108)', 'rgb(15,82,186)', 'rgb(137,207,240)']\n",
    "\n",
    "plot = boxplot_grouped(\n",
    "    dataframe=file,\n",
    "    x_col='FinancialYear',\n",
    "    y_col='ConditionScore',\n",
    "    group_col='VehicleType',\n",
    "    colors=colors,\n",
    "    title=None,\n",
    "    x_title='Financial year',\n",
    "    y_title='Condition score'\n",
    "    )\n",
    "\n",
    "# Export this plot as a vector graphic image:\n",
    "pio.write_image(plot, './images/condition_score_variation_boxplot.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot just the variation of mean. Dispersion illustration is\n",
    "# missing in it but the trend is a lot clearer to follow:\n",
    "\n",
    "x = pivot_ConditionScore.FinancialYear\n",
    "trace1 = go.Scatter(\n",
    "    name='Heavy Goods Vehicle',\n",
    "    x=x,\n",
    "    y=pivot_ConditionScore['mean']['Heavy Goods Vehicle'],\n",
    "    mode = 'markers',\n",
    "    showlegend=True,\n",
    "    marker=dict(\n",
    "        color=colors[0],\n",
    "        size=10,\n",
    "        line=dict(width=1.5)))\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    name='Light Goods Vehicle',\n",
    "    x=x,\n",
    "    y=pivot_ConditionScore['mean']['Light Goods Vehicle'],\n",
    "    mode = 'markers',\n",
    "    showlegend=True,\n",
    "    marker=dict(\n",
    "        color=colors[1],\n",
    "        size=10,\n",
    "        line=dict(width=1.5)))\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    name='Personnel Vehicle',\n",
    "    x=x,\n",
    "    y=pivot_ConditionScore['mean']['Personnel Vehicle'],\n",
    "    mode = 'markers',\n",
    "    showlegend=True,\n",
    "   marker=dict(\n",
    "        color=colors[2],\n",
    "        size=10,\n",
    "        line=dict(width=1.5)))\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout= go.Layout(\n",
    "    title= None,\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title= 'Financial year',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Average annual condition score',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    legend=dict(orientation=\"h\",\n",
    "               x=0, y=-0.3)\n",
    "    )\n",
    "fig= go.Figure(data=data, layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "# Export this plot as a vector graphic image:\n",
    "pio.write_image(fig, './images/condition_score_variation_means.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "Lets get the total condition score per manufacturer per financial year in each vehicle category. We know that heavy and light goods vehicles consistently score worse than personnel vehicles thus binning them all together would not be fair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean condition score divided by the amount of inspected\n",
    "# vehicles every year in each vehicle tyope category:\n",
    "\n",
    "av_annual_score_each_manufacturer = file.groupby(['FinancialYear', 'VehicleType','Manufacturer']).agg({'ConditionScore':sum, 'VehicleID':'count'}).reset_index()\n",
    "av_annual_score_each_manufacturer.columns = ['FinancialYear', 'VehicleType', 'Manufacturer', 'TotalScore', 'VehicleCount']\n",
    "av_annual_score_each_manufacturer['ConditionScorePerVehicle'] = av_annual_score_each_manufacturer.TotalScore/av_annual_score_each_manufacturer.VehicleCount\n",
    "\n",
    "# Inspecting a few top records:\n",
    "\n",
    "av_annual_score_each_manufacturer.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the dynamics of each manufacturers total annual score for each vehicle type:\n",
    "\n",
    "fig = grouped_plotter(\n",
    "    av_annual_score_each_manufacturer,\n",
    "    plot_title='Total annual score for each manufacturer',\n",
    "    kwargs=None)\n",
    "# Export this plot as a vector graphic image:\n",
    "pio.write_image(fig, './images/total_annual_score_each_manufacturer.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the dynamics of each manufacturers annual\n",
    "# score per vehicle (total score divided by the amount of vehicles inspected) for each vehicle type:\n",
    "\n",
    "fig = grouped_plotter(\n",
    "    av_annual_score_each_manufacturer,\n",
    "    plot_title='Annual score per vehicle for each manufacturer',\n",
    "    kwargs={\n",
    "        \n",
    "        'x': 'FinancialYear',\n",
    "        'x_title':'Financial year',\n",
    "        'y': 'ConditionScorePerVehicle',\n",
    "        'y_title':'Annual score per vehicle',\n",
    "        'group_column_inner': 'Manufacturer',\n",
    "        'group_column_outer': 'VehicleType'})\n",
    "# Export this plot as a vector graphic image:\n",
    "pio.write_image(fig, './images/annual_score_per_vehicle_each_manufacturer.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "This answer will be answered without the need to use computing.\n",
    "\n",
    "## Q5\n",
    "\n",
    "To answer this question we need to investigate the standard error of sample mean and confidence intervals for our records. We know that the larger the sample size the closer is the sample estimator to the population estimator being that mean, variance or any other parameter.\n",
    "\n",
    "The question we have to ask is how precise do we have to know the value of records. This precision value has to be significantly smaller than the trend we are trying to illustrate. If we are judging which car manufacturer is better and worse based on each of their mean annual score we have to know that the standard error of mean is smaller than the difference between mean scores for those manufacturers.\n",
    "\n",
    "To be able to tell if the sample size is large enough I will compare the differences in annual score values per vehicle category (scatter graph for Q2) against the standard error of sample means for a corresponding year. If it is a small proportion of the corresponding difference then it means it is precise enough to make a valid comparison and increasing sample size would not benefit analysis.\n",
    "\n",
    "To evaluate the standard error of sample mean and standard error of sample standard deviation of an unknown distribution (our score distributions are not normal for sure, see a few quick histogram plots underneath) I will use non parametric bootstrap to build up a histogram of resampled means and its standard deviation is the value of sample's standar error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distribution of Personnel Vehicles in 2001 and 2002 (score and its frequency) for all manufacturers:\n",
    "\n",
    "for i in file.FinancialYear.unique()[0:2]:\n",
    "    file[(file.VehicleType=='Personnel Vehicle')&(file.FinancialYear== i)].ConditionScore.hist(bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distribution of Light Goods Vehicles in 2001 and 2002  (score and its frequency) for all manufacturers:\n",
    "\n",
    "for i in file.FinancialYear.unique()[0:2]:\n",
    "    file[(file.VehicleType=='Light Goods Vehicle')&(file.FinancialYear== i)].ConditionScore.hist(bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distribution of Heavy Goods Vehicles in 2001 and 2002 (score and its frequency) for all manufacturers:\n",
    "\n",
    "for i in file.FinancialYear.unique()[0:2]:\n",
    "    file[(file.VehicleType=='Heavy Goods Vehicle')&(file.FinancialYear== i)].ConditionScore.hist(bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below I defined a numpy implementation of bootstrap standard error and confidence interval calculation\n",
    "\n",
    "\n",
    "def nonparam_bootstrap_st_err(data, repeats):\n",
    "    \"\"\" Returns a bootstrap standard error\n",
    "        of sample mean for a given dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    resampled_set = numpy.random.choice(data,(len(x), repeats))\n",
    "    distrib_of_means = resampled_set.mean(axis=0)\n",
    "    return numpy.std(distrib_of_means)\n",
    "\n",
    "def nonparam_bootstrap_conf_int(data, repeats, conf_interval=95):\n",
    "    \"\"\" Returns a bootstrap confidence interval of a given dataset.\n",
    "        return type is a float that corresponds to half the range\n",
    "        of values covered under the selected \"conf_interval\" so that\n",
    "        the mean can be quoted sample mean +/-\"return of this function\". \n",
    "    \"\"\"\n",
    "    \n",
    "    resampled_set = numpy.random.choice(data,(len(x), repeats))\n",
    "    distrib_of_means = resampled_set.mean(axis=0)\n",
    "    distrib_of_means.sort()\n",
    "    percentile = numpy.percentile(distrib_of_means, [0.5*(100-conf_interval),100-(0.5*(100-conf_interval))])\n",
    "    return 0.5*(percentile[1]-percentile[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create annual_means dataframe and populate extra columns for corresponding\n",
    "# standard errors of means and confidence intervals (90 % but we can choose the tolerance) \n",
    "\n",
    "annual_means = pandas.concat([pivot_ConditionScore['FinancialYear'],pivot_ConditionScore['mean']],axis=1)\n",
    "annual_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes the originally given big datafile and the one with\n",
    "# mean scores per each year per vehicle category and creates extra columns. Those\n",
    "# columns are then filled by values generated by feeding values of the original\n",
    "# dataframe iteratively \n",
    "\n",
    "def SE_calculator(df1, df2, repeats, func, conf_interval=95):\n",
    "    df1 = df1.copy()\n",
    "    max_ind = df1.shape[1]\n",
    "    df1['HGV_st_err'] = numpy.NaN\n",
    "    df1['LGV_st_err'] = numpy.NaN\n",
    "    df1['PV_st_err'] = numpy.NaN\n",
    "    for i in df1.index:\n",
    "        for j,k in zip(df1.columns[1:max_ind], df1.columns[max_ind:]):\n",
    "            x = df2[(df2.VehicleType==j)&\n",
    "                 (df2.FinancialYear== df1.loc[i, 'FinancialYear'])].ConditionScore.values\n",
    "            df1.at[i, k] =func(x, repeats)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function SE_calculator on annual_means dataframe using data in files dataframe:\n",
    "\n",
    "st_error_df = SE_calculator(annual_means, file, repeats=10000, func=nonparam_bootstrap_st_err)\n",
    "st_error_df = st_error_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspectiong proportions of each vehicle in a category every year:\n",
    "\n",
    "proportions = file.groupby(['FinancialYear', 'VehicleType']).agg({'VehicleID':'count'}).reset_index()\n",
    "proportions = proportions.pivot_table(values='VehicleID', index='FinancialYear', columns='VehicleType').reset_index()\n",
    "proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some calculations and plots for written task (government spending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file into pandas frame:\n",
    "\n",
    "spend_df = pandas.read_csv('./Data/gov_spend.csv')\n",
    "spend_df.head() # just to look at the top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating differences in transport\n",
    "# infrastructure ecpenditures in successive years:\n",
    "\n",
    "spend_df['diff_transport_spend'] = spend_df.transport_spend.diff()\n",
    "print('average spending increase: ', str(spend_df.diff_transport_spend.mean()))\n",
    "\n",
    "# Calculate percentage of transport expenses in total spent amount (in %):\n",
    "spend_df['ratio_transport_spend'] = 100*(spend_df.transport_spend/spend_df.total_spend)\n",
    "print('average spending proportion to total spent: ', str(spend_df.ratio_transport_spend.mean()))\n",
    "spend_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting spending on Transport and population vs time:\n",
    "\n",
    "trace1 = plotly.graph_objs.Scatter(\n",
    "    x=spend_df.Year,\n",
    "    y=spend_df.population,\n",
    "    name='UK population'\n",
    ")\n",
    "trace2 = plotly.graph_objs.Scatter(\n",
    "    x=spend_df.Year,\n",
    "    y=spend_df.transport_spend,\n",
    "    name='Government spending on transport infrastructure'\n",
    ")\n",
    "\n",
    "data = [trace2,trace1]\n",
    "layout= plotly.graph_objs.Layout(\n",
    "    title= None,\n",
    "    showlegend=True,\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title= 'Financial year',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Expenditure bn (GBP)',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    legend=dict(orientation=\"h\",\n",
    "               x=0.2, y=-0.2),\n",
    "        )\n",
    "\n",
    "fig = plotly.graph_objs.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='stacked-bar')\n",
    "\n",
    "plotly.io.write_image(fig, './images/transport_spending.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting spending percentage on Transport vs time:\n",
    "\n",
    "trace1 = plotly.graph_objs.Scatter(\n",
    "    x=spend_df.Year,\n",
    "    y=spend_df.ratio_transport_spend,\n",
    "    name='UK population'\n",
    ")\n",
    "\n",
    "layout= plotly.graph_objs.Layout(\n",
    "    title= None,\n",
    "    showlegend=False,\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title= 'Financial year',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Proportion of transport expenditure to total (%)',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    legend=dict(orientation=\"h\",\n",
    "               x=0.2, y=-0.2),\n",
    "        )\n",
    "\n",
    "fig2 = plotly.graph_objs.Figure(data=[trace1], layout=layout)\n",
    "plotly.offline.iplot(fig2, filename='stacked-bar')\n",
    "\n",
    "plotly.io.write_image(fig2, './images/transport_spending_percentage.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dev1)",
   "language": "python",
   "name": "dev1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
